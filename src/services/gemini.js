// Smart Chat AI with Full Memory & Debating Council Architecture
// Implements Tier-3 Deliberative Reasoning

import { PRECOMPUTED_DEBATES } from '../data/preComputedDebates';

// --- 1. PROMPTS & SCHEMAS (STRICT) ---

const SYSTEM_STRUCTURAL = `
You are a structural analyst for LLM training. 
Analyze the user's raw input and Normalize it into a clean JSON structure.
Infer missing values based on standard defaults (e.g., AdamW, mixed precision).

OUTPUT CONFIDENCE MAP: Rate your confidence (0.0-1.0) in each inference.
Output strictly valid JSON.
`;

const SCHEMA_STRUCTURAL = `
{
  "explicit": { "gpu": "...", "model": "...", "batch": 4, ... },
  "inferred": { "optimizer": "...", "precision": "...", ... },
  "confidence_map": { 
      "gpu_specs": 0.9, 
      "optimizer_choice": 0.5, 
      "dataset_quality": 0.1 
  },
  "assumptions": ["Assumed single GPU", "Assumed data is float32..."],
  "unknowns": ["dataset_quality"]
}
`;

// ‚ö° ADVERSARIAL PROMPTS (Financial Penalty)
const PROMPT_HARDWARE = `
Role: HARDWARE PESSIMIST.
OBJECTIVE: You are financially penalized $1,000 for every "False Negative" (saying a run is safe when it actually crashes). 
Your goal is to find ANY reason this configuration will fail.

Calculate WORST-CASE VRAM usage:
1. Model Weights: params * 2 (fp16) or 4 (fp32)
2. Optimizer: params * 2 (AdamW 8-bit) or 8 (standard)
3. Gradients: params * 2 (fp16) or 4 (fp32)
4. Activations: Batch * Seq * Layers * Hidden * Multiplier (approx)

Input Config: {{CONFIG}}

Output strict JSON:
{
  "agent_role": "pessimist",
  "memory_analysis": {
      "model_mem_gb": number,
      "optim_mem_gb": number,
      "activation_mem_gb": number,
      "total_vram_usage": number,
      "peak_usage_gb": number
  },
  "max_safe_batch": integer,
  "oom_probability": number (0.0-1.0),
  "risk_assessment": {
      "level": "low"|"med"|"high",
      "killer_factor": "string (what kills the run)"
  },
  "reasoning": "string" 
}
`;

const PROMPT_DYNAMICS = `
Role: TRAINING OPTIMIST.
OBJECTIVE: You are an ambitious engineer. You fail if the model trains too slowly or doesn't converge.
Your goal is to maximize throughput and learning speed, even if it pushes hardware limits.

Check: Batch size vs Dataset size, Learning Rate sanity, Epochs vs Overfitting.

Input Config: {{CONFIG}}

Output strict JSON:
{
  "agent_role": "optimist",
  "training_dynamics": {
      "training_time_hours": number,
      "recommended_batch": integer,
      "convergence_risk": "low"|"med"|"high"
  },
  "optimization_strategy": {
      "use_gradient_checkpointing": boolean,
      "use_offloading": boolean
  },
  "reasoning": "string",
  "recommendation": "string"
}
`;

const PROMPT_REFEREE = `
Role: DEBATE REFEREE.
Compare the Hardware (Pessimist) and Dynamics (Optimist) reports.

RULES:
1. If Hardware.max_safe_batch < Dynamics.recommended_batch => "BATCH_CONFLICT"
2. If Hardware.oom_probability > 0.5 AND Dynamics.convergence_risk == "low" => "RISK_DISAGREEMENT"

Assign an Agreement Score (1-10). If < 7, the debate continues.

Hardware Report: {{HARDWARE}}
Dynamics Report: {{DYNAMICS}}

Output strict JSON:
{
  "agreement_score": number,
  "conflicts": [
      {"type": "BATCH_CONFLICT", "severity": "high", "description": "..."}
  ],
  "concerns": ["Hardware says OOM but Dynamics recommends larger batch"],
  "synthesis_direction": "Trust Hardware on limits, Dynamics on learning"
}
`;

const PROMPT_CHIEF = `
Role: CHIEF SYNTHESIST.
Write the final report for the user based on the debate.

Input:
- Config: {{CONFIG}}
- Hardware: {{HARDWARE}}
- Dynamics: {{DYNAMICS}}
- Referee: {{REFEREE}}

Output a structured JSON report (NOT Markdown).
Break down recommendations into individual actionable cards.

IMPORTANT: If any critical information is missing or unclear, add "open_questions" to ask the user.
Examples of missing info: dataset quality (clean/noisy?), training goal (speed vs accuracy?), deployment target, etc.
If all critical info is present, set open_questions to an empty array.

Output strict JSON:
{
  "verdict": "One sentence summary (e.g. RISKY or SAFE or CRITICAL)",
  "debate_summary": "The Pessimist argued X, but the Optimist countered with Y.",
  "recommendations": [
      { "category": "Batch Size", "advice": "Reduce to 4 to avoid OOM." },
      { "category": "Learning Rate", "advice": "Keep at 5e-5." },
      { "category": "Optimization", "advice": "Enable gradient checkpointing." }
  ],
  "open_questions": [
      { "topic": "Dataset Quality", "question": "Is your data clean or scraped from the web?", "why": "Noisy data may need preprocessing or larger batch sizes." },
      { "topic": "Training Goal", "question": "Optimizing for speed or quality?", "why": "This affects batch size and learning rate choices." }
  ],
  "confidence_score": 85
}
`;

// --- 2. CORE FUNCTIONS ---

// Helper for JSON calls
async function callGeminiJSON(systemPrompt, userPrompt, apiKey) {
    try {
        const response = await fetch(
            `https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent?key=${apiKey}`,
            {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    contents: [{ role: 'user', parts: [{ text: userPrompt }] }],
                    systemInstruction: { parts: [{ text: systemPrompt + "\nRespond with VALID JSON ONLY." }] },
                    generationConfig: {
                        temperature: 0.2,
                        responseMimeType: "application/json" // Force JSON mode
                    }
                })
            }
        );

        if (!response.ok) throw new Error('API Error');
        const data = await response.json();
        return JSON.parse(data.candidates[0].content.parts[0].text);
    } catch (e) {
        console.error("JSON Error", e);
        return null; // Handle gracefully
    }
}

// Helper for Text calls
async function callGeminiText(systemPrompt, userPrompt, apiKey) {
    try {
        const response = await fetch(
            `https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent?key=${apiKey}`,
            {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    contents: [{ role: 'user', parts: [{ text: userPrompt }] }],
                    systemInstruction: { parts: [{ text: systemPrompt }] },
                    generationConfig: { temperature: 0.4 }
                })
            }
        );
        const data = await response.json();
        return data.candidates[0].content.parts[0].text;
    } catch (e) {
        return "Error generating report.";
    }
}

// --- 3. ORCHESTRATOR ---

export async function analyzeWithMultiAI(rawConfig, apiKey, history) {
    // ‚ö° DEMO SHORTCUT: Check for Pre-computed Traces
    // If the config matches a demo preset exactly, return the instantaneous trace.
    if (rawConfig.modelFamily === "Llama-3" && rawConfig.modelParams === "8B" && rawConfig.batchSize >= 32) {
        // Return demo trace 1 (Llama-3 OOM)
        return {
            success: true,
            // Use precomputed JSON synthesis directly
            content: PRECOMPUTED_DEBATES["llama-3-8b"].synthesis,
            breakdown: PRECOMPUTED_DEBATES["llama-3-8b"]
        };
    }
    if (rawConfig.modelFamily === "Mistral" && rawConfig.modelParams === "7B" && rawConfig.precision === "fp16") {
        // Return demo trace 2 (Mistral Risk)
        return {
            success: true,
            // Use precomputed JSON synthesis directly
            content: PRECOMPUTED_DEBATES["mistral-7b"].synthesis,
            breakdown: PRECOMPUTED_DEBATES["mistral-7b"]
        };
    }

    // --- REAL API EXECUTION ---

    // Stage 1: Normalize
    const normalized = await callGeminiJSON(SYSTEM_STRUCTURAL + SCHEMA_STRUCTURAL, JSON.stringify(rawConfig), apiKey);
    if (!normalized) return { success: false, error: "Failed to parse config. Please check your inputs." };

    const configStr = JSON.stringify(normalized);

    // Stage 2: Parallel Council (Hardware vs Dynamics)
    const [hardware, dynamics] = await Promise.all([
        callGeminiJSON(PROMPT_HARDWARE.replace('{{CONFIG}}', configStr), "Find failure points.", apiKey),
        callGeminiJSON(PROMPT_DYNAMICS.replace('{{CONFIG}}', configStr), "Optimize throughput.", apiKey)
    ]);

    if (!hardware || !dynamics) return { success: false, error: "Council agents failed to respond." };

    // Stage 3: Referee
    // We do a simplified single-pass referee here for speed, but logic allows looping
    const referee = await callGeminiJSON(
        PROMPT_REFEREE
            .replace('{{HARDWARE}}', JSON.stringify(hardware))
            .replace('{{DYNAMICS}}', JSON.stringify(dynamics)),
        "Arbitrate the conflict.",
        apiKey
    );

    // Stage 4: Synthesis (Now JSON)
    const finalReport = await callGeminiJSON(
        PROMPT_CHIEF
            .replace('{{CONFIG}}', configStr)
            .replace('{{HARDWARE}}', JSON.stringify(hardware))
            .replace('{{DYNAMICS}}', JSON.stringify(dynamics))
            .replace('{{REFEREE}}', JSON.stringify(referee)),
        "Synthesize final report.",
        apiKey
    );

    // Return structured object for UI
    return {
        success: true,
        content: finalReport, // Now a JSON Object
        breakdown: {
            normalized,
            council: { hardware, dynamics }, // Mapped to new UI structure
            referee, // Used for 'Debate' tab
            debate: { // Simulated minimal debate object for real runs
                rounds: referee?.conflicts?.length > 0 ?
                    [{ speaker: "Referee", text: `Major Conflict: ${referee.conflicts[0].type}`, type: "conflict" }] :
                    [{ speaker: "Referee", text: "Council reached consensus.", type: "agreement" }]
            }
        }
    };
}

// --- 4. INTENT-DRIVEN ADAPTIVE CHAT ---
// Stage 0: INTENT DETECTOR - What does user actually want?
// Stage 1: CONTEXT BUILDER - Light extraction of relevant facts
// Stage 2: RESPONSE ROUTER - Route to appropriate mode
// Stage 3: ADAPTIVE RESPONSE - Different styles for different intents

// ===== STAGE 0: INTENT DETECTOR =====
const PROMPT_INTENT = `
Analyze what the user ACTUALLY wants from this message.

USER MESSAGE: "{{MESSAGE}}"
RECENT CONTEXT: {{HISTORY}}

Classify into ONE intent:
- "explain": Wants understanding ("why", "how does X work", "can you explain")
- "advise": Wants actionable tips ("should I", "what's better", "how do I fix")
- "review": Wants feedback on their setup ("does this look good", "is this right")
- "brainstorm": Open exploration ("what else could I try", "any ideas")
- "debug": Has specific error ("not working", "crashes", "OOM", error messages)
- "casual": Just chatting ("thanks", "hi", "got it")
- "suggest_config": Wants specific config to try ("suggest a config", "what should I put", "let's try analyzer", "give me values", "fill in the config", "what config")

Return JSON ONLY:
{"intent": "explain", "confidence": 0.9, "key_topic": "gradient accumulation"}
`;

// ===== RESPONSE MODE PROMPTS =====

const MODE_EXPLAIN = `
You are a senior ML engineer EXPLAINING a concept to a colleague.

CONTEXT: {{CONTEXT}}
USER ASKED: "{{MESSAGE}}"
TOPIC: {{TOPIC}}

CRITICAL RULES:
1. Reference SPECIFIC values from the context (batch size, GPU model, VRAM, etc.) in your explanation.
2. NEVER use template placeholders like %s, %d, {value}, or [number]. Always use actual values.
3. If a value is missing from context, say "not specified" instead of using a placeholder.

STYLE:
- Write in natural paragraphs (NOT bullets)
- ALWAYS cite actual numbers from their config when relevant
- Use analogies that relate to their specific hardware
- 2-3 paragraphs max (150-250 words)
- End with: "Does that make sense?" or "Want me to go deeper on [aspect]?"

PERSONALITY: Enthusiastic teacher, casual but knowledgeable.
Think: explaining to a smart colleague over coffee, not writing documentation.

Example (if they asked about gradient accumulation with batch_size=4):
"With your batch size of 4, gradient accumulation basically lets you simulate a larger batch without actually loading more samples into memory at once. Think of it like this: instead of processing 32 samples simultaneously (which your RTX 3090 can't handle), you process 4 samples 8 times and accumulate the gradients before updating weights..."

Now explain naturally, citing their specific config values:
`;

const MODE_ADVISE = `
You are a senior ML engineer giving QUICK, ACTIONABLE advice.

CONTEXT: {{CONTEXT}}
USER ASKED: "{{MESSAGE}}"

CRITICAL RULES:
1. Reference SPECIFIC values from their config and analysis results (OOM %, VRAM usage, etc.).
2. NEVER use template placeholders like %s, %d, {value}, or [number]. Always use actual values.
3. If a value is missing, say "you didn't specify X" instead of using a placeholder.

STYLE:
- 3-5 bullets MAX
- Each bullet: [Do this] ‚Üí [Why/Expected result with NUMBERS]
- Cite their actual hardware and the analysis predictions
- Be opinionated ("Definitely try X" not "You might consider X")
- Max 100 words total

ANALYZER NUDGE (IMPORTANT):
If you suggest ANY config changes (batch size, learning rate, model, etc.), 
END your response with: "üí° Update these in the config and hit **Analyze** to see how it affects memory/training time!"

PERSONALITY: Direct, confident, no hedging.
Think: Slack message from a busy but helpful colleague.

Example (if their analysis shows 85% OOM risk on RTX 3090):
"Your current config shows 85% OOM risk - here's the fix:
‚Ä¢ Drop batch_size from 32 to 8 ‚Üí should bring you under 20GB VRAM
‚Ä¢ Enable gradient_checkpointing ‚Üí saves another ~4GB
‚Ä¢ Keep bf16 ‚Üí you're already optimized there

üí° Update these in the config and hit **Analyze** to verify!"

Now advise, citing their specific numbers:
`;

const MODE_REVIEW = `
You are a senior ML engineer REVIEWING someone's training setup.

CONTEXT: {{CONTEXT}}
USER SETUP: "{{MESSAGE}}"

STYLE:
- Start by acknowledging something specific they're doing right
- Then 1-2 concerns (quote their exact words)
- Then 1 opportunity they might be missing
- 2-3 short paragraphs (NOT bullets)
- Max 150 words

PERSONALITY: Supportive code reviewer. Find issues but don't be harsh.
Think: PR review comment from a senior engineer.

ANALYZER NUDGE: If you suggest any changes, end with:
"If you make these tweaks, run **Analyze** again to double-check before training."

Example opening:
"Your noise injection approach is interesting - I've seen similar tricks work well for low-resource languages..."

Now review:
`;

const MODE_BRAINSTORM = `
You are BRAINSTORMING with a colleague about ML training approaches.

CONTEXT: {{CONTEXT}}
USER EXPLORING: "{{MESSAGE}}"

STYLE:
- Throw out 3-5 ideas, mix of safe and creative
- Label each: [Safe Bet], [Experimental], or [Wild Idea]
- Short explanation for each (15 words max)
- Be excited and collaborative ("Ooh what about...", "Here's a wild one...")
- Max 150 words

PERSONALITY: Creative collaborator, not judge. Encourage exploration.

ANALYZER NUDGE: After listing ideas, end with:
"Pick one and I'll help you set up the config ‚Üí then run **Analyze** to validate before training!"

Example:
"Some directions to consider:

[Safe Bet] Character-level tokenization - proven for morphologically rich text
[Experimental] Curriculum learning - start with clean samples, add noise gradually  
[Wild Idea] Back-translation augmentation - translate Akkadian‚ÜíEnglish‚ÜíAkkadian to triple your data

Pick one and I'll help set up the config ‚Üí run **Analyze** before you train!"

Now brainstorm:
`;

const MODE_DEBUG = `
You are DEBUGGING a training issue with a colleague.

CONTEXT: {{CONTEXT}}
PROBLEM: "{{MESSAGE}}"

STYLE:
- Be systematic: Hypothesis ‚Üí Evidence to check ‚Üí Fix if confirmed
- Ask for specific info you need (logs, config values, error messages)
- Numbered steps (1, 2, 3)
- Don't guess - ask clarifying questions first if needed
- Max 120 words

PERSONALITY: Calm debugger. Patient, methodical.
Think: Pair programming. Specify what to check.

Example:
(Visualize the stack trace and guide them through a fix.)

Now debug:
`;

const MODE_CASUAL = `
You are a friendly ML colleague just chatting.

RECENT CONVERSATION: {{HISTORY}}
USER SAID: "{{MESSAGE}}"

STYLE:
- 1-2 sentences max
- Warm and conversational
- If they said thanks, acknowledge and offer next step
- If they're confused, gently clarify
- No formal structure needed

Example responses:
- "No problem! Let me know if you hit any snags."
- "Haha yeah, training runs are the worst kind of suspense."
- "Got it - want me to dig deeper on any part?"

Now respond naturally:
`;

// ===== NEW: SUGGEST CONFIG MODE =====
const MODE_SUGGEST_CONFIG = `
You are helping the user configure their training run. Based on the conversation, suggest SPECIFIC values.

CURRENT CONFIG: {{CONTEXT}}
CONVERSATION HISTORY: {{HISTORY}}
USER MESSAGE: "{{MESSAGE}}"

YOUR JOB: Suggest exact configuration values to try in the analyzer.

OUTPUT FORMAT (follow exactly):

üéØ **Ready to Test This Configuration:**

| Field | Suggested Value | Why |
|-------|-----------------|-----|
| Model | [exact model name] | [brief reason] |
| GPU | [GPU model] | [brief reason] |
| VRAM | [number]GB | [brief reason] |
| Batch Size | [number] | [brief reason] |
| Learning Rate | [number] | [brief reason] |
| Epochs | [number] | [brief reason] |
| Dataset Size | [number] | [brief reason] |
| Precision | [fp16/bf16/fp32] | [brief reason] |
| LoRA Rank | [number or N/A] | [brief reason] |
| Sequence Length | [number] | [brief reason] |
| Gradient Checkpointing | [Yes/No] | [brief reason] |

üìù **Additional Notes (paste this):**
\`\`\`
[Write 2-3 sentences summarizing the key context: dataset domain, goals, constraints, any special techniques discussed]
\`\`\`

‚ö° **Expected Outcome:**
- [What this config should achieve]
- [Any risks to watch for]

üëâ Click "Analyze" to see the detailed breakdown!

RULES:
- Be SPECIFIC with numbers (not "small" but "4")
- Base suggestions on what was discussed in the conversation
- If something wasn't discussed, use sensible defaults for their hardware
- The Additional Notes should capture domain-specific context (e.g., "Akkadian translation", "low-resource language")
`;

// ===== MAIN FUNCTION: INTENT-DRIVEN CHAT =====
export async function smartChat(userMsg, history, config, analysis, configHistory, apiKey, image = null) {
    const fullContext = buildFullContext(config, analysis, history);
    const userQuestion = userMsg || (image ? "Analyze this uploaded image" : "Help me with my configuration");
    const recentHistory = history.slice(-3).map(m => `${m.role}: ${m.content?.slice(0, 100)}`).join('\n');

    try {
        // ===== STAGE 0: DETECT INTENT =====
        let intent = { intent: 'advise', confidence: 0.5, key_topic: '' };
        try {
            const intentPrompt = PROMPT_INTENT
                .replace('{{MESSAGE}}', userQuestion)
                .replace('{{HISTORY}}', recentHistory);
            const intentResponse = await callGeminiText(intentPrompt, userQuestion, apiKey);
            intent = safeParseJSON(intentResponse) || intent;
        } catch (e) {
            console.error('Intent detection failed:', e);
            // Fallback: quick heuristic
            intent = detectIntentHeuristic(userQuestion);
        }

        console.log(`[Intent Detected]: ${intent.intent} (${intent.confidence})`);

        // ===== STAGE 1: BUILD LIGHT CONTEXT =====
        const contextSummary = buildContextSummary(config, analysis, userQuestion);

        // ===== STAGE 2: ROUTE TO RESPONSE MODE =====
        let responsePrompt;
        // Get longer history for config suggestions
        const fullHistory = history.slice(-8).map(m => `${m.role}: ${m.content?.slice(0, 200)}`).join('\n');

        switch (intent.intent) {
            case 'explain':
                responsePrompt = MODE_EXPLAIN
                    .replace('{{CONTEXT}}', contextSummary)
                    .replace('{{MESSAGE}}', userQuestion)
                    .replace('{{TOPIC}}', intent.key_topic || userQuestion);
                break;
            case 'review':
                responsePrompt = MODE_REVIEW
                    .replace('{{CONTEXT}}', contextSummary)
                    .replace('{{MESSAGE}}', userQuestion);
                break;
            case 'brainstorm':
                responsePrompt = MODE_BRAINSTORM
                    .replace('{{CONTEXT}}', contextSummary)
                    .replace('{{MESSAGE}}', userQuestion);
                break;
            case 'debug':
                responsePrompt = MODE_DEBUG
                    .replace('{{CONTEXT}}', contextSummary)
                    .replace('{{MESSAGE}}', userQuestion);
                break;
            case 'casual':
                responsePrompt = MODE_CASUAL
                    .replace('{{HISTORY}}', recentHistory)
                    .replace('{{MESSAGE}}', userQuestion);
                break;
            case 'suggest_config':
                responsePrompt = MODE_SUGGEST_CONFIG
                    .replace('{{CONTEXT}}', contextSummary)
                    .replace('{{HISTORY}}', fullHistory)
                    .replace('{{MESSAGE}}', userQuestion);
                break;
            case 'advise':
            default:
                responsePrompt = MODE_ADVISE
                    .replace('{{CONTEXT}}', contextSummary)
                    .replace('{{MESSAGE}}', userQuestion);
                break;
        }

        // ===== STAGE 3: GENERATE ADAPTIVE RESPONSE WITH RETRY =====
        // NOTE: Putting prompt in the user message, not systemInstruction
        // systemInstruction causes "Internal error" on preview models
        const responseParts = [{ text: responsePrompt }];
        if (image && image.base64) {
            responseParts.push({
                inlineData: {
                    mimeType: image.mimeType || 'image/jpeg',
                    data: image.base64
                }
            });
        }

        // Compress history to reduce input tokens (last 3 exchanges = 6 messages max)
        const compressedHistory = history.slice(-6).map(m => ({
            role: m.role || 'user',
            parts: [{ text: (m.content || '').slice(0, 500) }] // Truncate long messages
        }));

        // Retry logic with exponential backoff
        const MAX_RETRIES = 3;
        let lastError = null;

        for (let attempt = 1; attempt <= MAX_RETRIES; attempt++) {
            try {
                const response = await fetch(
                    `https://generativelanguage.googleapis.com/v1beta/models/gemini-3-flash-preview:generateContent?key=${apiKey}`,
                    {
                        method: 'POST',
                        headers: { 'Content-Type': 'application/json' },
                        body: JSON.stringify({
                            contents: [
                                ...compressedHistory,
                                { role: 'user', parts: responseParts }
                            ],
                            generationConfig: {
                                temperature: intent.intent === 'brainstorm' ? 0.7 : 0.5,
                                maxOutputTokens: 2048, // Increased from 800 to prevent truncation
                                topP: 0.95,
                                topK: 40
                            },
                            // Safety settings to prevent false positives on technical ML content
                            safetySettings: [
                                { category: "HARM_CATEGORY_HARASSMENT", threshold: "BLOCK_NONE" },
                                { category: "HARM_CATEGORY_HATE_SPEECH", threshold: "BLOCK_NONE" },
                                { category: "HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold: "BLOCK_NONE" },
                                { category: "HARM_CATEGORY_DANGEROUS_CONTENT", threshold: "BLOCK_NONE" }
                            ]
                        })
                    }
                );

                const data = await response.json();

                // Debug logging for finishReason
                const candidate = data.candidates?.[0];
                console.log('[SmartChat] Response:', {
                    attempt,
                    finishReason: candidate?.finishReason,
                    textLength: candidate?.content?.parts?.[0]?.text?.length,
                    hasError: !!data.error
                });

                // Handle API errors
                if (data.error) {
                    console.error(`[SmartChat] API Error (attempt ${attempt}):`, data.error);
                    if (attempt < MAX_RETRIES && (data.error.code === 500 || data.error.message?.includes('internal'))) {
                        // Exponential backoff: 1s, 2s, 4s
                        await new Promise(r => setTimeout(r, 1000 * Math.pow(2, attempt - 1)));
                        continue;
                    }
                    throw new Error(data.error.message || 'API Error');
                }

                if (!candidate) {
                    if (attempt < MAX_RETRIES) {
                        await new Promise(r => setTimeout(r, 1000 * Math.pow(2, attempt - 1)));
                        continue;
                    }
                    throw new Error('No response candidate');
                }

                let responseText = candidate.content?.parts?.[0]?.text || '';

                // Handle truncation
                if (candidate.finishReason === 'MAX_TOKENS') {
                    console.warn('[SmartChat] Response truncated due to MAX_TOKENS');
                    responseText += '\n\n*[Response truncated - ask a follow-up for more details]*';
                }

                // Handle safety filter
                if (candidate.finishReason === 'SAFETY') {
                    console.warn('[SmartChat] Response blocked by safety filter');
                    return {
                        success: true,
                        content: '‚ö†Ô∏è Response blocked by safety filter. Try rephrasing your question.',
                        debug: { finishReason: 'SAFETY' }
                    };
                }

                return {
                    success: true,
                    content: responseText,
                    debug: {
                        intent: intent.intent,
                        confidence: intent.confidence,
                        topic: intent.key_topic,
                        finishReason: candidate.finishReason,
                        attempt
                    }
                };

            } catch (attemptError) {
                console.error(`[SmartChat] Attempt ${attempt} failed:`, attemptError.message);
                lastError = attemptError;
                if (attempt < MAX_RETRIES) {
                    await new Promise(r => setTimeout(r, 1000 * Math.pow(2, attempt - 1)));
                }
            }
        }

        // All retries failed
        console.error('[SmartChat] All retries exhausted:', lastError);
        return {
            success: true,
            content: `‚ö†Ô∏è Chat temporarily unavailable after ${MAX_RETRIES} attempts. Please try again.\n\n(Error: ${lastError?.message || 'Unknown'})`,
            debug: { error: lastError?.message }
        };

    } catch (e) {
        console.error('SmartChat error:', e);
        return { success: false, error: e.message };
    }
}

// ===== HELPER: Heuristic Intent Detection (Fallback) =====
function detectIntentHeuristic(message) {
    const msg = message.toLowerCase();

    if (/\b(why|how does|explain|what is|what's|can you explain)\b/.test(msg)) {
        return { intent: 'explain', confidence: 0.8, key_topic: '' };
    }
    if (/\b(error|crash|fail|oom|not working|broken)\b/.test(msg)) {
        return { intent: 'debug', confidence: 0.9, key_topic: '' };
    }
    if (/\b(look good|is this right|review|check|does this)\b/.test(msg)) {
        return { intent: 'review', confidence: 0.7, key_topic: '' };
    }
    if (/\b(what else|ideas|brainstorm|options|alternatives|could try)\b/.test(msg)) {
        return { intent: 'brainstorm', confidence: 0.7, key_topic: '' };
    }
    if (/^(thanks|thank you|got it|ok|okay|hi|hello|hey)\b/.test(msg)) {
        return { intent: 'casual', confidence: 0.9, key_topic: '' };
    }
    // NEW: Detect config suggestion requests
    if (/\b(suggest.*(config|values)|what.*(put|values|config)|let's try|try.*(analyzer|analysis)|give me.*config|fill.*config)\b/.test(msg)) {
        return { intent: 'suggest_config', confidence: 0.85, key_topic: '' };
    }
    return { intent: 'advise', confidence: 0.5, key_topic: '' };
}

// ===== HELPER: Build Context Summary =====
function buildContextSummary(config, analysis, userMessage) {
    const hasAnalysis = analysis && analysis.content && analysis.content.verdict;
    const breakdown = analysis?.breakdown;

    let summary = `User's Config: Model=${config.modelFamily || '?'}, GPU=${config.gpu || '?'} (${config.vram || '?'}GB VRAM), `;
    summary += `Batch=${config.batchSize || '?'}, LR=${config.learningRate || '?'}, Epochs=${config.epochs || '?'}, Precision=${config.precision || '?'}`;

    if (config.additionalNotes) {
        summary += `\nNotes: ${config.additionalNotes.slice(0, 200)}`;
    }

    if (hasAnalysis) {
        summary += `\n\n=== ANALYSIS RESULTS ===`;
        summary += `\nVerdict: ${analysis.content.verdict}`;

        // Add detailed hardware analysis data
        if (breakdown?.council?.hardware) {
            const hw = breakdown.council.hardware;
            if (hw.oom_probability !== undefined) {
                summary += `\nOOM Risk: ${Math.round(hw.oom_probability * 100)}%`;
            }
            if (hw.memory_analysis) {
                summary += `\nEstimated Peak VRAM: ${hw.memory_analysis.peak_usage_gb || '?'}GB`;
                summary += `\nModel Weights: ${hw.memory_analysis.model_weights_gb || '?'}GB`;
            }
            if (hw.reasoning) {
                summary += `\nHardware Analysis: ${hw.reasoning.slice(0, 150)}`;
            }
        }

        // Add recommendations if present
        if (analysis.content.recommendations && analysis.content.recommendations.length > 0) {
            summary += `\n\nKey Recommendations:`;
            analysis.content.recommendations.slice(0, 3).forEach(rec => {
                summary += `\n- ${rec.category}: ${rec.advice?.slice(0, 100) || rec}`;
            });
        }
    }

    return summary;
}

// ===== HELPER: Build Full Context (for reference) =====
function buildFullContext(config, analysis, history) {
    const hasAnalysis = analysis && analysis.content && analysis.content.verdict;

    return `
=== USER'S CONFIGURATION ===
Model: ${config.model || 'Not specified'}
GPU: ${config.gpu || 'Not specified'} (${config.vram || '?'}GB VRAM)
Batch Size: ${config.batchSize || '?'}, LR: ${config.learningRate || '?'}, Epochs: ${config.epochs || '?'}
Precision: ${config.precision || '?'}, LoRA Rank: ${config.loraRank || '?'}

=== ADDITIONAL NOTES ===
${config.additionalNotes || 'None'}

=== ANALYSIS STATUS ===
${hasAnalysis ? `Verdict: ${analysis.content.verdict}` : 'No analysis run yet'}
`;
}

// ===== HELPER: Safe JSON Parser =====
function safeParseJSON(text) {
    try {
        const jsonMatch = text.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
            return JSON.parse(jsonMatch[0]);
        }
        return null;
    } catch (e) {
        console.error('JSON parse error:', e);
        return null;
    }
}

export function demoChatResponse(userMessage, conversationHistory, config, analysisResult) {
    const msg = userMessage.toLowerCase();
    const hasAnalysis = analysisResult && analysisResult.content;
    const content = analysisResult?.content || {};

    // If asking about specific sections
    if (msg.includes('memory') || msg.includes('vram')) {
        if (hasAnalysis && analysisResult.breakdown?.council?.hardware) {
            const hw = analysisResult.breakdown.council.hardware;
            return `**Memory Analysis:**\n\n` +
                `- Estimated model weights: ~${hw.memory_analysis?.model_weights_gb || 'N/A'} GB\n` +
                `- Peak VRAM usage: ~${hw.memory_analysis?.peak_usage_gb || 'N/A'} GB\n` +
                `- OOM Risk: ${Math.round((hw.oom_probability || 0) * 100)}%\n\n` +
                `${hw.risk_assessment?.reason || 'No critical memory issues detected.'}`;
        }
        return "Run an analysis first to see memory breakdown.";
    }

    if (msg.includes('assumption') || msg.includes('estimate')) {
        if (hasAnalysis && analysisResult.breakdown?.normalized?.assumptions) {
            return `**Current Assumptions:**\n\n` +
                analysisResult.breakdown.normalized.assumptions.map(a => `- ${a}`).join('\n') +
                `\n\nThese are inferred from your config. Update **Additional Notes** with corrections and re-run.`;
        }
        return "Run an analysis first to see assumptions.";
    }

    if (msg.includes('recommend') || msg.includes('advice') || msg.includes('suggest')) {
        if (content.recommendations && content.recommendations.length > 0) {
            return `**Recommendations:**\n\n` +
                content.recommendations.map(r => `- **${r.category}:** ${r.advice}`).join('\n');
        }
        return "Run an analysis first to get recommendations.";
    }

    if (msg.includes('verdict') || msg.includes('safe') || msg.includes('risk')) {
        if (content.verdict) {
            return `**Verdict:** ${content.verdict}\n\nConfidence: ${content.confidence_score || 'N/A'}%`;
        }
        return "Run an analysis first to see the verdict.";
    }

    if (msg.includes('question') || msg.includes('missing')) {
        if (content.open_questions && content.open_questions.length > 0) {
            return `**Open Questions:**\n\n` +
                content.open_questions.map(q => `- **${q.topic}:** ${q.question}`).join('\n') +
                `\n\nAnswer these in **Additional Notes** and click **Analyze Configuration** for a refined analysis.`;
        }
        return "No open questions ‚Äî the analysis has all the info it needs!";
    }

    // General fallback
    if (hasAnalysis) {
        return `I have your analysis ready. Ask me about:\n\n` +
            `- **Memory usage** and VRAM estimates\n` +
            `- **Recommendations** for optimization\n` +
            `- **Assumptions** I made\n` +
            `- **Open questions** I need answered\n\n` +
            `Or ask any ML training question!\n\n` +
            `*Note: For full AI chat, add your Gemini API key.*`;
    }

    return `**Demo Mode**\n\nI can help once you run an analysis. Click **Analyze Configuration** first!\n\n` +
        `For full AI chat capabilities, add your Gemini API key in the header.`;
}

export default { analyzeWithMultiAI, smartChat, demoChatResponse };
